

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.cluster import KMeans
import math
import streamlit as st
import os
from PIL import Image



st.set_page_config(layout="wide")
st.title('Identify Target Audience Using Sampling Techniques')

file = st.file_uploader("Upload CSV", type="csv")

if file is not None:
    # Use pandas to read the file contents into a DataFrame
    data = pd.read_csv(file)
    st_df1  = pd.DataFrame(data)
    
    population_mean = st_df1['total_discount_received'].mean()
    
    st.markdown("")
    see_data = st.expander('You can click here to see the dataset first üëâ')
    with see_data:
        st.dataframe(data=st_df1)
    st.text('')
    
    
        
    st.sidebar.title("Findings")
    type_of_finding = st.sidebar.selectbox("Select one",('EDA', 'Clustering', 'Sampling Techniques'))   
        
    if type_of_finding == 'EDA':
        
       st.subheader("General Information about Dataset")
       row2_spacer1, row2_1, row2_spacer2, row2_2, row2_spacer3, row2_3, row2_spacer4, row2_4, row2_spacer5   = st.columns((.2, 1.6, .2, 1.6, .2, 1.6, .2, 1.6, .2))

       with row2_1:
           Total_Records = "üìâ No of Records: " +  str(286392) 
           st.markdown(Total_Records)
       with row2_2:
           no_of_var_df = "üî¢ No of Variables: " + str(36) 
           st.markdown(no_of_var_df)
       with row2_3:
           res_to_discount = "ü§ëResponse Rate for Discount : " + str(27.34) +"%"
           st.markdown(res_to_discount)
       with row2_4:
           red_rate = "üõçÔ∏è Redemption Rate: " + str(59.77) + "%"
           st.markdown(red_rate)

       folder_path = 'content/'
       file_names = os.listdir(folder_path)
       md_files = [f for f in file_names if f.endswith('.md')]
       selected_file = st.sidebar.selectbox('Select an MD file', md_files)

       with open(os.path.join(folder_path, selected_file), 'r') as f:
           file_contents = f.read()

       st.write(file_contents)
    
       img_folder_path = 'EDA Plots/'
       img_file_name = os.path.splitext(selected_file)[0] + '.png'
       img_path = os.path.join(img_folder_path, img_file_name)
        
       if os.path.isfile(img_path):
           img = Image.open(img_path)
           st.image(img, caption=img_file_name)
       else:
           st.write("No corresponding image file found.")
        
    elif type_of_finding == 'Clustering':
       st.write('No MD files available for this option.')
    
    elif type_of_finding == 'Sampling Techniques':
    
       agree = st.checkbox('Apply Sampling Techniques')

       if agree:
           def simple_random_sampling(data, sample_sizes):
               np.random.seed(42)
               samples = []
               for size,confidence_interval in sample_sizes:
                   indices = random.sample(range(len(data)),size)
                   srs_sample = data.iloc[indices]
                   srs_mean = srs_sample['total_discount_received'].mean()
                   srs_sd = np.std(srs_sample['total_discount_received'], ddof=1) 
                   srs_se = srs_sd / np.sqrt(size)
                   srs_absolute_error = abs(population_mean - srs_mean)
                   samples.append(['Simple Random Sampling',confidence_interval, size, srs_absolute_error, srs_se])
               return samples

           def stratified_sampling(data, sample_sizes):
               np.random.seed(42)
               data2 = data
               samples = []
               data2.loc[data2['Gender_M'] == 1, 'Customer_Strata'] = 1
               data2.loc[data2['Gender_M'] != 1, 'Customer_Strata'] = 0

               for size,confidence_interval in sample_sizes:
                   split = StratifiedShuffleSplit(n_splits=1, test_size=size)
                   for x, y in split.split(data2, data2['Customer_Strata']):
                       str_sample = data2.iloc[y].sort_values(by='cust_id')
                       str_mean = str_sample['total_discount_received'].mean()
                       str_se = np.std(str_sample['total_discount_received'], ddof=1) / np.sqrt(size)
                       str_absolute_error = abs(population_mean - str_mean)
                   samples.append(['Stratified Sampling',confidence_interval, size, str_absolute_error, str_se])
               return samples 

           def systematic_sampling(data, sample_sizes):
               np.random.seed(42)
               samples = []
               for size,confidence_interval in sample_sizes:
                   step = len(data) // size
                   start = np.random.randint(0, step)
                   indices = np.arange(start, len(data), step = step)
                   sys_sample = data.iloc[indices]
                   sys_mean = sys_sample['total_discount_received'].mean()
                   sys_sd = np.std(sys_sample['total_discount_received'], ddof=1) 
                   sys_se = sys_sd / np.sqrt(size)
                   sys_absolute_error = abs(population_mean - sys_mean)
                   samples.append(['Systematic Sampling',confidence_interval, size, sys_absolute_error, sys_se])
               return samples
            

           def cluster_sampling(data, sample_sizes):
               samples = []

               cluster1=data.loc[data['cluster'] == 0]
               cluster2=data.loc[data['cluster'] == 1]
               cluster3=data.loc[data['cluster'] == 3]

               clusters = []
               clusters.append(cluster1)
               clusters.append(cluster2)
               clusters.append(cluster3)

               random.seed(42)
               selected_cluster = random.choice(clusters)

               for size,confidence_interval in sample_sizes:
                   indices = random.sample(range(len(selected_cluster)), size)
                   cluster_sample = selected_cluster.iloc[indices]
                   cluster_mean = cluster_sample['total_discount_received'].mean()
                   cluster_se = np.std(cluster_sample['total_discount_received'], ddof=1) / np.sqrt(size)
                   cluster_absolute_error = abs(population_mean - cluster_mean)
                   samples.append(['Cluster Sampling', confidence_interval,size, cluster_absolute_error, cluster_se])
               return samples


           def sampling_pipeline(data, sample_sizes):
               srs_samples = simple_random_sampling(data, sample_sizes)
               str_samples = stratified_sampling(data, sample_sizes)
               sys_samples = systematic_sampling(data, sample_sizes)
               cls_samples = cluster_sampling(data, sample_sizes)

               # Combine results into DataFrame
               train_set = srs_samples  + sys_samples + cls_samples + str_samples
               df = pd.DataFrame(train_set, columns=['Sampling Technique','Confidence Interval','Sample Size', 'Absolute Error', 'Standard Error'])
               return df



           # Calculate the sample size for each confidence interval
           sample_sizes = []
           # Set the given values
           z_scores = [(1.645, '90%'), (1.96, '95%'), (2.576, '99%')]
           margin_of_error = 0.05
           population_proportion = 0.5
           population_size = len(st_df1) #9521

           for z, confidence_interval in z_scores:
               sample_size = (((z**2) * population_proportion * (1 - population_proportion)) / (margin_of_error**2)) / (1+ ((z**2) * population_proportion * (1 - population_proportion))/((margin_of_error**2)*population_size))
               sample_size = math.ceil(sample_size) # Round up to the nearest integer
               sample_sizes.append((sample_size, confidence_interval))

           #print("Sample sizes needed for 90%, 95%, and 99% confidence intervals:", sample_sizes)

           df = st_df1
           result_table = sampling_pipeline(df, sample_sizes)
           #st.write(result_table)
           st.table(result_table)
